{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import tarfile\n",
    "\n",
    "ROOT_DIR = 'D:\\\\datasets\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subimages(image_path, destination_dir, patch_width=160, patch_height=160):\n",
    "    image_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    img = Image.open(image_path)\n",
    "    img_width, img_height = img.size\n",
    "    img_height -= img_height % patch_height\n",
    "    img_width -= img_width % patch_width\n",
    "    k = 1\n",
    "    for i in range(0, img_height, patch_height):\n",
    "        for j in range(0, img_width, patch_width):\n",
    "            box = (j, i, j + patch_width, i + patch_height)\n",
    "            patch = Image.new('RGB', (patch_height, patch_width), 255)\n",
    "            patch.paste(img.crop(box))\n",
    "            patch.save(f'{destination_dir}{image_name}_{k}.jpg')\n",
    "            k += 1\n",
    "\n",
    "def copy_image(image_path, destination_dir):\n",
    "    image_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    img = Image.open(image_path)\n",
    "    img.save(f'{destination_dir}{image_name}.jpg')\n",
    "\n",
    "def create_compressed(img, destination_path, level, rotation):\n",
    "    if rotation:\n",
    "        img = img.rotate(rotation)\n",
    "    img.save(destination_path, format='jpeg', quality=level)\n",
    "\n",
    "def create_noisified(img, destination_path, level, rotation):\n",
    "    if rotation:\n",
    "        img = img.rotate(rotation)\n",
    "    img = np.array(img).astype(np.float32)\n",
    "    img /= 255.\n",
    "    img += level * np.random.normal(size=img.shape)\n",
    "    img = np.clip(img, 0., 1.)\n",
    "    img = Image.fromarray(np.uint8(img * 255))\n",
    "    img.save(destination_path)\n",
    "\n",
    "def create_downscaled(img, destination_path, factor, rotation):\n",
    "    if rotation:\n",
    "        img = img.rotate(rotation)\n",
    "    img_width, img_height = img.size\n",
    "    img = img.resize((int(img_width / factor), int(img_height / factor)), Image.BICUBIC)\n",
    "    img = img.resize((img_width, img_height), Image.BICUBIC)\n",
    "    img.save(destination_path)\n",
    "\n",
    "def preprocess_bsds500(tar_filename):\n",
    "    \"\"\"\n",
    "    Complete pre-process of the whole dataset and saving in a tar file.\n",
    "    Not optimized for easier understanding (run only once anyway).\n",
    "    \"\"\"\n",
    "    dataset_dir = f'{ROOT_DIR}bsds500\\\\'\n",
    "    temp_dir = f'{ROOT_DIR}temp\\\\'\n",
    "    # Delete and create temp dir\n",
    "    if os.path.exists(temp_dir):\n",
    "        shutil.rmtree(temp_dir)\n",
    "    # Write sub-images in temp/targets/train dir and copy others in corresponding temp/dirs\n",
    "    for local_dir in ['train', 'test', 'val']:\n",
    "        destination_dir = f'{temp_dir}targets\\\\{local_dir}\\\\'\n",
    "        os.makedirs(destination_dir)\n",
    "        igms_paths = glob.glob(f'{dataset_dir}{local_dir}\\\\*.jpg')\n",
    "        for i, image_path in enumerate(igms_paths):\n",
    "            if local_dir == 'train':\n",
    "                create_subimages(image_path, destination_dir)\n",
    "            else:\n",
    "                copy_image(image_path, destination_dir)\n",
    "    # Preprocess sub-images, create a downscaled, noisified and compressed version for each train and val image\n",
    "    for local_dir in ['train', 'val']:\n",
    "        destination_dir = f'{temp_dir}data\\\\{local_dir}\\\\'\n",
    "        os.makedirs(destination_dir)\n",
    "        igms_paths = glob.glob(f'{temp_dir}targets\\\\{local_dir}\\\\*.jpg')\n",
    "        for image_path in igms_paths:\n",
    "            image_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "            img = Image.open(image_path)\n",
    "            create_noisified(img, f'{destination_dir}{image_name}_n.jpg',\n",
    "                              [.15, .25, .50][random.randint(0, 2)], random.randint(0, 3) * 90 if local_dir == 'train' else 0)\n",
    "            create_downscaled(img, f'{destination_dir}{image_name}_d.jpg',\n",
    "                              random.randint(2, 4), random.randint(0, 3) * 90 if local_dir == 'train' else 0)\n",
    "            create_compressed(img, f'{destination_dir}{image_name}_c.jpg',\n",
    "                              random.randint(1, 4) * 10, random.randint(0, 3) * 90 if local_dir == 'train' else 0)\n",
    "    # Preprocess all test sets\n",
    "    igms_paths = glob.glob(f'{temp_dir}targets\\\\test\\\\*.jpg')\n",
    "    for image_path in igms_paths:\n",
    "        image_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        img = Image.open(image_path)\n",
    "        for local_dir in ['noise_15', 'noise_25', 'noise_50',\n",
    "                      'upscale_2', 'upscale_3', 'upscale_4',\n",
    "                      'compress_10', 'compress_20', 'compress_30', 'compress_40']:\n",
    "            destination_dir = f'{temp_dir}data\\\\test\\\\{local_dir}\\\\'\n",
    "            if not os.path.exists(destination_dir):\n",
    "                os.makedirs(destination_dir)\n",
    "            action, level = local_dir.split('_')\n",
    "            if action == 'noise':\n",
    "                create_noisified(img, f'{destination_dir}{image_name}.jpg', int(level) / 100., 0)\n",
    "            if action == 'upscale':\n",
    "                create_downscaled(img, f'{destination_dir}{image_name}.jpg', int(level), 0)\n",
    "            if action == 'compress':\n",
    "                create_compressed(img, f'{destination_dir}{image_name}.jpg', int(level), 0)\n",
    "    # Create tar file with temp dir\n",
    "    with tarfile.open(tar_filename, \"w:gz\") as tar:\n",
    "        tar.add(temp_dir, arcname=os.path.basename(temp_dir))\n",
    "    # Delete temp dir\n",
    "    shutil.rmtree(temp_dir)\n",
    "\n",
    "def load_bsds500(batch_size):\n",
    "    tar_filename = f'{ROOT_DIR}bsds500.tgz'\n",
    "    # If tar file does not exists\n",
    "    if not os.path.exists(tar_filename):\n",
    "        preprocess_bsds500(tar_filename)\n",
    "    # Create tf datasets from tar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_bsds500(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
